name: CPI Crawler

on:
  schedule:
    - cron: '0 0 1 * *'  # Run at 00:00 on the first day of every month
  workflow_dispatch:  # Allow manual trigger

jobs:
  crawl:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      id-token: write
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run CPI crawler
      run: python crawler/run_cpi_crawlers.py
      
    - name: Commit changes if new data
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/raw/vn_cpi.csv data/raw/jp_cpi.csv data/cleaned/cpi_data.csv
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update CPI data [skip ci]" && git push)
        
    - name: Store data to cloud
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}    
      run: python store_data/cpi_store_data.py 